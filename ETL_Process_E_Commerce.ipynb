{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e3d801",
   "metadata": {},
   "source": [
    "# Phase 1: Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d92f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Connect to the MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='your password',\n",
    "    database='ecommerce_db'\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Extract data from CSV files\n",
    "products_df = pd.read_csv('~/Documents/ETL_Project/products_data.csv')\n",
    "orders_df = pd.read_csv('~/Documents/ETL_Project/orders_data.csv')\n",
    "users_df = pd.read_csv('~/Documents/ETL_Project/users_data.csv')\n",
    "\n",
    "# Extract data from Excel files\n",
    "payments_df = pd.read_excel('~/Documents/ETL_Project/payments_data.xlsx', engine='openpyxl')\n",
    "reviews_df = pd.read_excel('~/Documents/ETL_Project/reviews_data.xlsx', engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb011bf9",
   "metadata": {},
   "source": [
    "## What’s Happening Here?\n",
    "* We start by importing the necessary libraries: pandas for data manipulation and mysql.connector to connect to the MySQL database.\n",
    "* We then establish a connection to the MySQL database using the mysql.connector.connect method.\n",
    "* The pd.read_csv and pd.read_excel functions are used to extract data from CSV and Excel files, respectively. This data is loaded into DataFrames, which are versatile and powerful data structures in Python that allow us to easily manipulate and analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3327f583",
   "metadata": {},
   "source": [
    "# Phase 2: Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1500d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in each dataframe as needed\n",
    "products_df = products_df.fillna({\n",
    "    'Description': 'No description available',\n",
    "    'Category': 'Miscellaneous'\n",
    "})\n",
    "\n",
    "orders_df = orders_df.fillna({\n",
    "    'TotalAmount': 0.0\n",
    "})\n",
    "\n",
    "payments_df = payments_df.fillna({\n",
    "    'PaymentMethod': 'Unknown',\n",
    "    'Amount': 0.0,\n",
    "    'PaymentDate': pd.Timestamp('1970-01-01')\n",
    "})\n",
    "\n",
    "reviews_df = reviews_df.fillna({\n",
    "    'ReviewText': 'no review'\n",
    "})\n",
    "\n",
    "# Handling invalid emails in the Users DataFrame\n",
    "def generate_unique_email(index):\n",
    "    return f'unknown_{index}@example.com'\n",
    "\n",
    "users_df['Email'] = users_df.apply(lambda row: row['Email'] if '@' in row['Email'] and '.' in row['Email'] else generate_unique_email(row.name), axis=1)\n",
    "\n",
    "# Ensure all prices are numeric\n",
    "products_df['Price'] = pd.to_numeric(products_df['Price'], errors='coerce').fillna(0.0).round(2)\n",
    "\n",
    "# Ensure all stock quantities are numeric\n",
    "products_df['StockQuantity'] = pd.to_numeric(products_df['StockQuantity'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Standardize and convert date formats to string\n",
    "orders_df['OrderDate'] = pd.to_datetime(orders_df['OrderDate'], errors='coerce', format='%Y-%m-%d').fillna(pd.Timestamp('1970-01-01'))\n",
    "orders_df['OrderDate'] = orders_df['OrderDate'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "payments_df['PaymentDate'] = pd.to_datetime(payments_df['PaymentDate'], errors='coerce', format='%Y-%m-%d').fillna(pd.Timestamp('1970-01-01'))\n",
    "payments_df['PaymentDate'] = payments_df['PaymentDate'].dt.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde36f4",
   "metadata": {},
   "source": [
    "## What’s Happening Here?\n",
    "* Filling Missing Values: We use the fillna method to replace missing values with default values. For example, if the Description or Category fields are missing in the products_df DataFrame, we fill them with 'No description available' and 'Miscellaneous', respectively.\n",
    "* Handling Invalid Emails: We define a function generate_unique_email to create a placeholder email for records with invalid or missing email addresses.\n",
    "* Ensuring Numeric Data: We convert the Price and StockQuantity fields in products_df to numeric values. Any non-numeric data is replaced with a default value (0.0 for prices, 0 for stock quantities).\n",
    "* Standardizing Dates: Dates in orders_df and payments_df are standardized to a consistent format (YYYY-MM-DD) and any invalid dates are replaced with a default placeholder date (1970-01-01).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb88b67",
   "metadata": {},
   "source": [
    "# Phase 3: Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d49135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all records from Payments, Reviews, Orders, Users, and Products tables\n",
    "tables_to_clear = ['Payments', 'Reviews', 'Orders', 'Users', 'Products']\n",
    "for table in tables_to_clear:\n",
    "    cursor.execute(f\"DELETE FROM {table}\")\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into Products table\n",
    "product_id_mapping = {}\n",
    "for index, row in products_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Products (ProductName, Description, Price, StockQuantity, Category)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\", (row['ProductName'], row['Description'], row['Price'], row['StockQuantity'], row['Category']))\n",
    "    product_id_mapping[index] = cursor.lastrowid\n",
    "conn.commit()\n",
    "\n",
    "# Insert Users Data and Map Old UserIDs to New UserIDs\n",
    "user_id_mapping = {}\n",
    "for index, row in users_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Users (UserName, Email, Address, Password)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (row['UserName'], row['Email'], row['Address'], row['Password']))\n",
    "    # Mapping old UserID index to the new UserID\n",
    "    user_id_mapping[index + 1] = cursor.lastrowid\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "# Update Orders Data with New UserIDs\n",
    "orders_df['UserID'] = orders_df['UserID'].map(user_id_mapping)\n",
    "\n",
    "\n",
    "# Insert data into Orders table\n",
    "order_id_mapping = {}\n",
    "for index, row in orders_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Orders (UserID, OrderDate, TotalAmount)\n",
    "        VALUES (%s, %s, %s)\n",
    "    \"\"\", (row['UserID'], row['OrderDate'], row['TotalAmount']))\n",
    "    order_id_mapping[index] = cursor.lastrowid\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "# Map the new OrderIDs to payments_df\n",
    "payments_df['OrderID'] = payments_df.index.map(order_id_mapping)\n",
    "\n",
    "\n",
    "\n",
    "# Insert data into Payments table\n",
    "for index, row in payments_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Payments (OrderID, PaymentMethod, PaymentDate, Amount)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (row['OrderID'], row['PaymentMethod'], row['PaymentDate'], row['Amount']))\n",
    "conn.commit()\n",
    "\n",
    "# Map the new UserIDs and ProductIDs to reviews_df\n",
    "reviews_df['UserID'] = reviews_df['UserID'].map(user_id_mapping)\n",
    "reviews_df['ProductID'] = reviews_df.index.map(product_id_mapping)\n",
    "\n",
    "# Filter out rows with invalid ratings\n",
    "reviews_df['Rating'] = pd.to_numeric(reviews_df['Rating'], errors='coerce')\n",
    "reviews_df = reviews_df.dropna(subset=['Rating', 'ProductID', 'UserID'])\n",
    "\n",
    "\n",
    "# Insert data into Reviews table\n",
    "for index, row in reviews_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Reviews (ProductID, UserID, Rating, ReviewText)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (row['ProductID'], row['UserID'], row['Rating'], row['ReviewText']))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54a7960",
   "metadata": {},
   "source": [
    "## What’s Happening Here?\n",
    "* Clearing Existing Data: Before inserting new data, we delete all records from the target tables (Payments, Reviews, Orders, Users, and Products) to avoid conflicts and ensure we’re working with fresh data.\n",
    "* Inserting Data: We iterate through each DataFrame, row by row, and insert the data into the corresponding table in the MySQL database. The cursor.execute method is used to run SQL INSERT statements with the data from each row.\n",
    "* Mapping IDs: As we insert data, we map old IDs to new ones. For instance, the UserID in the Orders table is updated to match the new UserID in the Users table after insertion.\n",
    "* Filtering and Loading Reviews: We filter out any invalid reviews (e.g., those with missing ratings) before inserting them into the Reviews table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d56d5",
   "metadata": {},
   "source": [
    "## Final Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc66882-3ee3-4341-b4d2-35b913ad3de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products verification passed: 60 records.\n",
      "Users verification passed: 10 records.\n",
      "Orders verification passed: 60 records.\n",
      "Payments verification passed: 60 records.\n",
      "Reviews verification passed: 51 records.\n"
     ]
    }
   ],
   "source": [
    "# Final Verification\n",
    "def verify_table_count(table_name, expected_count):\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    if count == expected_count:\n",
    "        print(f\"{table_name} verification passed: {count} records.\")\n",
    "    else:\n",
    "        print(f\"{table_name} verification failed: Expected {expected_count}, but found {count}.\")\n",
    "\n",
    "# Verify Products table\n",
    "verify_table_count('Products', len(products_df))\n",
    "\n",
    "# Verify Users table\n",
    "verify_table_count('Users', len(users_df))\n",
    "\n",
    "# Verify Orders table\n",
    "verify_table_count('Orders', len(orders_df))\n",
    "\n",
    "# Verify Payments table\n",
    "verify_table_count('Payments', len(payments_df))\n",
    "\n",
    "# Verify Reviews table\n",
    "verify_table_count('Reviews', len(reviews_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e402e",
   "metadata": {},
   "source": [
    "## What’s Happening Here?\n",
    "* We define a verify_table_count function that checks the number of records in each table and compares it to the expected count from the DataFrames.\n",
    "* This ensures that all data has been correctly inserted into the database and that no records were missed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
